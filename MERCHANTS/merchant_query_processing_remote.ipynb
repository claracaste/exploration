{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.10/site-packages (12.0.1)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /opt/conda/lib/python3.10/site-packages (from pyarrow) (1.26.4)\n",
      "\u001b[33mWARNING: Error parsing requirements for fsspec: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/fsspec-2023.6.0.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: fastparquet in /opt/conda/lib/python3.10/site-packages (2024.5.0)\n",
      "Requirement already satisfied: pandas>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from fastparquet) (2.1.4)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from fastparquet) (1.26.4)\n",
      "Requirement already satisfied: cramjam>=2.3 in /opt/conda/lib/python3.10/site-packages (from fastparquet) (2.8.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from fastparquet) (2023.6.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from fastparquet) (23.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.5.0->fastparquet) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.5.0->fastparquet) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.5.0->fastparquet) (2024.1)\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/fsspec-2023.6.0.dist-info/METADATA'\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: awswrangler in /opt/conda/lib/python3.10/site-packages (3.8.0)\n",
      "Requirement already satisfied: boto3<2.0.0,>=1.20.32 in /opt/conda/lib/python3.10/site-packages (from awswrangler) (1.34.51)\n",
      "Requirement already satisfied: botocore<2.0.0,>=1.23.32 in /opt/conda/lib/python3.10/site-packages (from awswrangler) (1.34.51)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18 in /opt/conda/lib/python3.10/site-packages (from awswrangler) (1.26.4)\n",
      "Requirement already satisfied: packaging<25.0,>=21.1 in /opt/conda/lib/python3.10/site-packages (from awswrangler) (23.2)\n",
      "Requirement already satisfied: pandas<3.0.0,>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from awswrangler) (2.1.4)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from awswrangler) (12.0.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.4.0 in /opt/conda/lib/python3.10/site-packages (from awswrangler) (4.5.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0.0,>=1.20.32->awswrangler) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from boto3<2.0.0,>=1.20.32->awswrangler) (0.10.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<2.0.0,>=1.23.32->awswrangler) (2.9.0)\n",
      "Requirement already satisfied: urllib3<2.1,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore<2.0.0,>=1.23.32->awswrangler) (1.26.18)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0.0,>=1.2.0->awswrangler) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0.0,>=1.2.0->awswrangler) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<2.0.0,>=1.23.32->awswrangler) (1.16.0)\n",
      "\u001b[33mWARNING: Error parsing requirements for fsspec: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/fsspec-2023.6.0.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: s3fs in /opt/conda/lib/python3.10/site-packages (2024.6.0)\n",
      "Requirement already satisfied: aiobotocore<3.0.0,>=2.5.4 in /opt/conda/lib/python3.10/site-packages (from s3fs) (2.12.2)\n",
      "Collecting fsspec==2024.6.0.* (from s3fs)\n",
      "  Using cached fsspec-2024.6.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from s3fs) (3.9.5)\n",
      "Requirement already satisfied: botocore<1.34.52,>=1.34.41 in /opt/conda/lib/python3.10/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.34.51)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /opt/conda/lib/python3.10/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.16.0)\n",
      "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /opt/conda/lib/python3.10/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (0.11.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (4.0.3)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.34.52,>=1.34.41->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore<1.34.52,>=1.34.41->aiobotocore<3.0.0,>=2.5.4->s3fs) (2.9.0)\n",
      "Requirement already satisfied: urllib3<2.1,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore<1.34.52,>=1.34.41->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.26.18)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (3.7)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.34.52,>=1.34.41->aiobotocore<3.0.0,>=2.5.4->s3fs) (1.16.0)\n",
      "Using cached fsspec-2024.6.0-py3-none-any.whl (176 kB)\n",
      "\u001b[33mWARNING: Error parsing requirements for fsspec: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/fsspec-2023.6.0.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: fsspec\n",
      "  Attempting uninstall: fsspec\n",
      "\u001b[33m    WARNING: No metadata found in /opt/conda/lib/python3.10/site-packages\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: fsspec 2023.6.0\n",
      "\u001b[31mERROR: Cannot uninstall fsspec 2023.6.0, RECORD file not found. You might be able to recover from this via: 'pip install --force-reinstall --no-deps fsspec==2023.6.0'.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyarrow\n",
    "%pip install fastparquet\n",
    "%pip install awswrangler\n",
    "%pip install s3fs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import awswrangler as wr\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from io import StringIO\n",
    "import s3fs\n",
    "from fastparquet import ParquetFile\n",
    "# from cleodata.utils.secrets import get_secret\n",
    "# from cleodata.sources.sync.sync import SyncDataSource\n",
    "# boto3.setup_default_session(profile_name='DataScientist-878877078763')\n",
    "# redshift_source = SyncDataSource(\"data_exploration\", use_redshift=True, redshift_cluster=\"cleo-production-redshift\", redshift_db=\"cleo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_s3(path):\n",
    "    \"\"\"Read parquet files and combine them into a single dataframe\"\"\"\n",
    "    fs = s3fs.core.S3FileSystem()\n",
    "    all_paths_from_s3 = fs.glob(path=f\"{path}*.parquet\")\n",
    "\n",
    "    if len(all_paths_from_s3) > 0:\n",
    "        s3 = s3fs.S3FileSystem()\n",
    "        fp_obj = ParquetFile(\n",
    "            all_paths_from_s3, open_with=s3.open\n",
    "        )  # use s3fs as the filesystem\n",
    "        data = fp_obj.to_pandas()\n",
    "        return data\n",
    "    elif len(all_paths_from_s3)==1:\n",
    "        return pd.read_parquet(all_paths_from_s3[0])\n",
    "    else:\n",
    "        print(f\"Nothing found\")\n",
    "        print(f\"paths from a{all_paths_from_s3}\")\n",
    "    \n",
    "def read_csv_s3(bucket, key):\n",
    "    try:\n",
    "        s3 = boto3.client('s3')\n",
    "        obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "        df = pd.read_csv(obj['Body'])\n",
    "        return df\n",
    "    except ClientError as ex:\n",
    "        if ex.response['Error']['Code'] == 'NoSuchKey':\n",
    "            print(\"Key doesn't match. Please check the key value entered.\")\n",
    "\n",
    "def list_s3_flies(base_path):\n",
    "    fs = s3fs.core.S3FileSystem()\n",
    "    all_paths_from_s3 = fs.glob(path=f\"{base_path}*.parquet\")\n",
    "    return all_paths_from_s3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2024-05-14',\n",
       " '2024-05-15',\n",
       " '2024-05-16',\n",
       " '2024-05-17',\n",
       " '2024-05-18',\n",
       " '2024-05-19',\n",
       " '2024-05-20',\n",
       " '2024-05-21',\n",
       " '2024-05-22']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_date_s = '2024-05-14'\n",
    "end_date_s = '2024-05-22'\n",
    "date_range = pd.date_range(start=start_date_s, end=end_date_s)\n",
    "# Convert the date range to a list of strings\n",
    "date_list = date_range.strftime('%Y-%m-%d').tolist()\n",
    "date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-14 2024-05-14\n",
      "Loading s3://cleo-data-science/transaction_enrichment/experimental_data/caste/raw/trans_2024-05-14_2024-05-14\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/sagemaker-user/exploration/MERCHANTS/merchant_query_processing_remote.ipynb Cell 5\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://yd6plnil3hw8unc.studio.us-east-1.sagemaker.aws/home/sagemaker-user/exploration/MERCHANTS/merchant_query_processing_remote.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m path_file \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39ms3://cleo-data-science/transaction_enrichment/experimental_data/caste/raw/trans_\u001b[39m\u001b[39m{\u001b[39;00mstart_date\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mend_date\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://yd6plnil3hw8unc.studio.us-east-1.sagemaker.aws/home/sagemaker-user/exploration/MERCHANTS/merchant_query_processing_remote.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoading \u001b[39m\u001b[39m{\u001b[39;00mpath_file\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://yd6plnil3hw8unc.studio.us-east-1.sagemaker.aws/home/sagemaker-user/exploration/MERCHANTS/merchant_query_processing_remote.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m df_trans \u001b[39m=\u001b[39m read_from_s3(path_file)\n\u001b[1;32m      <a href='vscode-notebook-cell://yd6plnil3hw8unc.studio.us-east-1.sagemaker.aws/home/sagemaker-user/exploration/MERCHANTS/merchant_query_processing_remote.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m Loaded shape \u001b[39m\u001b[39m{\u001b[39;00mdf_trans\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://yd6plnil3hw8unc.studio.us-east-1.sagemaker.aws/home/sagemaker-user/exploration/MERCHANTS/merchant_query_processing_remote.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m path_file_processed \u001b[39m=\u001b[39m path_file\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39mraw/\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39mprocessed/\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39mpath_file\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39mraw/\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m1\u001b[39m]\n",
      "\u001b[1;32m/home/sagemaker-user/exploration/MERCHANTS/merchant_query_processing_remote.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://yd6plnil3hw8unc.studio.us-east-1.sagemaker.aws/home/sagemaker-user/exploration/MERCHANTS/merchant_query_processing_remote.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     s3 \u001b[39m=\u001b[39m s3fs\u001b[39m.\u001b[39mS3FileSystem()\n\u001b[1;32m      <a href='vscode-notebook-cell://yd6plnil3hw8unc.studio.us-east-1.sagemaker.aws/home/sagemaker-user/exploration/MERCHANTS/merchant_query_processing_remote.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     fp_obj \u001b[39m=\u001b[39m ParquetFile(\n\u001b[1;32m      <a href='vscode-notebook-cell://yd6plnil3hw8unc.studio.us-east-1.sagemaker.aws/home/sagemaker-user/exploration/MERCHANTS/merchant_query_processing_remote.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m         all_paths_from_s3, open_with\u001b[39m=\u001b[39ms3\u001b[39m.\u001b[39mopen\n\u001b[1;32m     <a href='vscode-notebook-cell://yd6plnil3hw8unc.studio.us-east-1.sagemaker.aws/home/sagemaker-user/exploration/MERCHANTS/merchant_query_processing_remote.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     )  \u001b[39m# use s3fs as the filesystem\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://yd6plnil3hw8unc.studio.us-east-1.sagemaker.aws/home/sagemaker-user/exploration/MERCHANTS/merchant_query_processing_remote.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     data \u001b[39m=\u001b[39m fp_obj\u001b[39m.\u001b[39;49mto_pandas()\n\u001b[1;32m     <a href='vscode-notebook-cell://yd6plnil3hw8unc.studio.us-east-1.sagemaker.aws/home/sagemaker-user/exploration/MERCHANTS/merchant_query_processing_remote.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m data\n\u001b[1;32m     <a href='vscode-notebook-cell://yd6plnil3hw8unc.studio.us-east-1.sagemaker.aws/home/sagemaker-user/exploration/MERCHANTS/merchant_query_processing_remote.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(all_paths_from_s3)\u001b[39m==\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fastparquet/api.py:790\u001b[0m, in \u001b[0;36mParquetFile.to_pandas\u001b[0;34m(self, columns, categories, filters, index, row_filter, dtypes)\u001b[0m\n\u001b[1;32m    786\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     parts \u001b[39m=\u001b[39m {name: (v \u001b[39mif\u001b[39;00m name\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m-catdef\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    788\u001b[0m                     \u001b[39melse\u001b[39;00m v[start:start \u001b[39m+\u001b[39m thislen])\n\u001b[1;32m    789\u001b[0m              \u001b[39mfor\u001b[39;00m (name, v) \u001b[39min\u001b[39;00m views\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m--> 790\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread_row_group_file(rg, columns, categories, index,\n\u001b[1;32m    791\u001b[0m                              assign\u001b[39m=\u001b[39;49mparts, partition_meta\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpartition_meta,\n\u001b[1;32m    792\u001b[0m                              row_filter\u001b[39m=\u001b[39;49msel, infile\u001b[39m=\u001b[39;49minfile)\n\u001b[1;32m    793\u001b[0m     start \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m thislen\n\u001b[1;32m    794\u001b[0m \u001b[39mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fastparquet/api.py:388\u001b[0m, in \u001b[0;36mParquetFile.read_row_group_file\u001b[0;34m(self, rg, columns, categories, index, assign, partition_meta, row_filter, infile)\u001b[0m\n\u001b[1;32m    385\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    386\u001b[0m f \u001b[39m=\u001b[39m infile \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopen(fn, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 388\u001b[0m core\u001b[39m.\u001b[39;49mread_row_group(\n\u001b[1;32m    389\u001b[0m     f, rg, columns, categories, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mschema, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcats,\n\u001b[1;32m    390\u001b[0m     selfmade\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mselfmade, index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m    391\u001b[0m     assign\u001b[39m=\u001b[39;49massign, scheme\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfile_scheme, partition_meta\u001b[39m=\u001b[39;49mpartition_meta,\n\u001b[1;32m    392\u001b[0m     row_filter\u001b[39m=\u001b[39;49mrow_filter\n\u001b[1;32m    393\u001b[0m )\n\u001b[1;32m    394\u001b[0m \u001b[39mif\u001b[39;00m ret:\n\u001b[1;32m    395\u001b[0m     \u001b[39mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fastparquet/core.py:645\u001b[0m, in \u001b[0;36mread_row_group\u001b[0;34m(file, rg, columns, categories, schema_helper, cats, selfmade, index, assign, scheme, partition_meta, row_filter)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[39mif\u001b[39;00m assign \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    644\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mGoing with pre-allocation!\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 645\u001b[0m read_row_group_arrays(file, rg, columns, categories, schema_helper,\n\u001b[1;32m    646\u001b[0m                       cats, selfmade, assign\u001b[39m=\u001b[39;49massign, row_filter\u001b[39m=\u001b[39;49mrow_filter)\n\u001b[1;32m    648\u001b[0m \u001b[39mfor\u001b[39;00m cat \u001b[39min\u001b[39;00m cats:\n\u001b[1;32m    649\u001b[0m     \u001b[39mif\u001b[39;00m cat \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m assign:\n\u001b[1;32m    650\u001b[0m         \u001b[39m# do no need to have partition columns in output\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fastparquet/core.py:615\u001b[0m, in \u001b[0;36mread_row_group_arrays\u001b[0;34m(file, rg, columns, categories, schema_helper, cats, selfmade, assign, row_filter)\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    613\u001b[0m remains\u001b[39m.\u001b[39mdiscard(name)\n\u001b[0;32m--> 615\u001b[0m read_col(column, schema_helper, file, use_cat\u001b[39m=\u001b[39;49mname\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m-catdef\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39min\u001b[39;49;00m out,\n\u001b[1;32m    616\u001b[0m          selfmade\u001b[39m=\u001b[39;49mselfmade, assign\u001b[39m=\u001b[39;49mout[name],\n\u001b[1;32m    617\u001b[0m          catdef\u001b[39m=\u001b[39;49mout\u001b[39m.\u001b[39;49mget(name\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m-catdef\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    618\u001b[0m          row_filter\u001b[39m=\u001b[39;49mrow_filter)\n\u001b[1;32m    620\u001b[0m \u001b[39mif\u001b[39;00m _is_map_like(schema_helper, column):\n\u001b[1;32m    621\u001b[0m     \u001b[39m# TODO: could be done in fast loop in _assemble_objects?\u001b[39;00m\n\u001b[1;32m    622\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m maps:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fastparquet/core.py:462\u001b[0m, in \u001b[0;36mread_col\u001b[0;34m(column, schema_helper, infile, use_cat, selfmade, assign, catdef, row_filter)\u001b[0m\n\u001b[1;32m    458\u001b[0m off \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m((cmd\u001b[39m.\u001b[39mdictionary_page_offset \u001b[39mor\u001b[39;00m cmd\u001b[39m.\u001b[39mdata_page_offset,\n\u001b[1;32m    459\u001b[0m            cmd\u001b[39m.\u001b[39mdata_page_offset))\n\u001b[1;32m    461\u001b[0m infile\u001b[39m.\u001b[39mseek(off)\n\u001b[0;32m--> 462\u001b[0m column_binary \u001b[39m=\u001b[39m infile\u001b[39m.\u001b[39;49mread(cmd\u001b[39m.\u001b[39;49mtotal_compressed_size)\n\u001b[1;32m    463\u001b[0m infile \u001b[39m=\u001b[39m encoding\u001b[39m.\u001b[39mNumpyIO(column_binary)\n\u001b[1;32m    464\u001b[0m rows \u001b[39m=\u001b[39m row_filter\u001b[39m.\u001b[39msum() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(row_filter, np\u001b[39m.\u001b[39mndarray) \u001b[39melse\u001b[39;00m cmd\u001b[39m.\u001b[39mnum_values\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fsspec/spec.py:1897\u001b[0m, in \u001b[0;36mAbstractBufferedFile.read\u001b[0;34m(self, length)\u001b[0m\n\u001b[1;32m   1894\u001b[0m \u001b[39mif\u001b[39;00m length \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1895\u001b[0m     \u001b[39m# don't even bother calling fetch\u001b[39;00m\n\u001b[1;32m   1896\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1897\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcache\u001b[39m.\u001b[39;49m_fetch(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloc, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloc \u001b[39m+\u001b[39;49m length)\n\u001b[1;32m   1899\u001b[0m logger\u001b[39m.\u001b[39mdebug(\n\u001b[1;32m   1900\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m read: \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m - \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1901\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1904\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache\u001b[39m.\u001b[39m_log_stats(),\n\u001b[1;32m   1905\u001b[0m )\n\u001b[1;32m   1906\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloc \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(out)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fsspec/caching.py:234\u001b[0m, in \u001b[0;36mReadAheadCache._fetch\u001b[0;34m(self, start, end)\u001b[0m\n\u001b[1;32m    232\u001b[0m end \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize, end \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocksize)\n\u001b[1;32m    233\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal_requested_bytes \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m end \u001b[39m-\u001b[39m start\n\u001b[0;32m--> 234\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfetcher(start, end)  \u001b[39m# new block replaces old\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart \u001b[39m=\u001b[39m start\n\u001b[1;32m    236\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mend \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/s3fs/core.py:2303\u001b[0m, in \u001b[0;36mS3File._fetch_range\u001b[0;34m(self, start, end)\u001b[0m\n\u001b[1;32m   2301\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fetch_range\u001b[39m(\u001b[39mself\u001b[39m, start, end):\n\u001b[1;32m   2302\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2303\u001b[0m         \u001b[39mreturn\u001b[39;00m _fetch_range(\n\u001b[1;32m   2304\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfs,\n\u001b[1;32m   2305\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbucket,\n\u001b[1;32m   2306\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey,\n\u001b[1;32m   2307\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mversion_id,\n\u001b[1;32m   2308\u001b[0m             start,\n\u001b[1;32m   2309\u001b[0m             end,\n\u001b[1;32m   2310\u001b[0m             req_kw\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreq_kw,\n\u001b[1;32m   2311\u001b[0m         )\n\u001b[1;32m   2313\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m ex:\n\u001b[1;32m   2314\u001b[0m         \u001b[39mif\u001b[39;00m ex\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m errno\u001b[39m.\u001b[39mEINVAL \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mpre-conditions\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m ex\u001b[39m.\u001b[39margs[\u001b[39m1\u001b[39m]:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/s3fs/core.py:2465\u001b[0m, in \u001b[0;36m_fetch_range\u001b[0;34m(fs, bucket, key, version_id, start, end, req_kw)\u001b[0m\n\u001b[1;32m   2463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2464\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mFetch: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, bucket, key, start, end)\n\u001b[0;32m-> 2465\u001b[0m \u001b[39mreturn\u001b[39;00m sync(fs\u001b[39m.\u001b[39;49mloop, _inner_fetch, fs, bucket, key, version_id, start, end, req_kw)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fsspec/asyn.py:91\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m asyncio\u001b[39m.\u001b[39mrun_coroutine_threadsafe(_runner(event, coro, result, timeout), loop)\n\u001b[1;32m     89\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     \u001b[39m# this loops allows thread to get interrupted\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m     \u001b[39mif\u001b[39;00m event\u001b[39m.\u001b[39;49mwait(\u001b[39m1\u001b[39;49m):\n\u001b[1;32m     92\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    608\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39;49macquire(\u001b[39mTrue\u001b[39;49;00m, timeout)\n\u001b[1;32m    325\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for one_date in date_list:\n",
    "    start_date = one_date\n",
    "    end_date = one_date\n",
    "    print(start_date, end_date)\n",
    "    #list the files that start with that pattern\n",
    "    path_file = f\"s3://cleo-data-science/transaction_enrichment/experimental_data/caste/raw/trans_{start_date}_{end_date}\"\n",
    "    print(f\"Loading {path_file}\")\n",
    "    df_trans = read_from_s3(path_file)\n",
    "    print(f\" Loaded shape {df_trans.shape}\")\n",
    "\n",
    "    path_file_processed = path_file.split('raw/')[0]+'processed/'+path_file.split('raw/')[1]\n",
    "    print(f\" Output {path_file_processed}\")\n",
    "    # Coalescing merchant names\n",
    "\n",
    "    #replace None with null\n",
    "    #replace Nan\n",
    "    # Replace empty spaces, None, and strings with only spaces with NaN\n",
    "    df_trans['merchant_name'] = df_trans['merchant_name'].replace(r'^\\s*$', np.nan, regex=True)\n",
    "    df_trans['merchant_name_plaid'] = df_trans['merchant_name_plaid'].replace(r'^\\s*$', np.nan, regex=True)\n",
    "\n",
    "\n",
    "    df_trans['merchant_name'] = df_trans['merchant_name'].replace('None',None)\n",
    "    df_trans['merchant_name'] = df_trans['merchant_name'].replace('',None)\n",
    "    df_trans['merchant_name'] = df_trans['merchant_name'].replace(' ',None)\n",
    "    df_trans['merchant_name_plaid'] = df_trans['merchant_name_plaid'].replace('', None)\n",
    "    df_trans['merchant_name_plaid'] = df_trans['merchant_name_plaid'].replace(' ', None)\n",
    "    df_trans['merchant_name'] = df_trans['merchant_name'].replace('None', None)\n",
    "\n",
    "    # create a column combined_merchant where we take the any merchant name : Cleo, or plaid, or counterparty \n",
    "    df_trans['merchant_name_combined'] = df_trans['merchant_name'].combine_first(df_trans['merchant_name_plaid'])\n",
    "    # if counterparty_type is merchant , user counterparty_name\n",
    "    #df_trans['merchant_name_combined'] = df_trans['merchant_name_combined'].combine_first(df_trans['counterparty_name'])\n",
    "    df_trans['merchant_name_combined'] = df_trans['merchant_name_combined'].combine_first(df_trans.apply(lambda row: row['counterparty_name'] if row['counterparty_type']=='merchant' else None, axis=1))\n",
    "    # Remove data without merchant name for training data\n",
    "    df_trans = df_trans[(~df_trans['merchant_name_combined'].isnull()) & ~df_trans['merchant_name_combined'].isin(['',' '])][:]\n",
    "    df_trans['merchant_name_combined_len'] = df_trans['merchant_name_combined'].apply(lambda x: len(x))\n",
    "    df_trans = df_trans[df_trans['merchant_name_combined_len']>=1]\n",
    "    df_trans.drop('merchant_name_combined_len', axis=1, inplace=True)\n",
    "\n",
    "    # Coalescing descriptions \n",
    "    # if original_description_plaid is empty use description\n",
    "    df_trans['description_combined'] = df_trans['original_description_plaid'].combine_first(df_trans['description'])\n",
    "    df_trans['len_description'] = df_trans['description_combined'].apply(lambda x: len(x))\n",
    "    df_trans = df_trans[df_trans['len_description'] >=2]\n",
    "    df_trans.drop('len_description', axis=1, inplace=True)\n",
    "    ##\n",
    "    #replace 'other' with ''\n",
    "    df_trans['payment_channel_processed'] = df_trans['payment_channel'].apply(lambda x: 'None' if x == 'other' else x)\n",
    "    # do some light processing to make strings shorter\n",
    "    df_trans['description_combined_processed'] =  df_trans['description_combined'].apply(lambda x: re.sub('\\\\\\\\+','\\\\\\\\',x))\n",
    "    df_trans['description_combined_processed'] =  df_trans['description_combined_processed'].apply(lambda x: re.sub(r'\\d{4,}', ' ', x))\n",
    "    df_trans['description_combined_processed'] =  df_trans['description_combined_processed'].apply(lambda x: re.sub(r'\\d{4,}', ' ', x))\n",
    "    df_trans['description_combined_processed'] =  df_trans['description_combined_processed'].apply(lambda x: re.sub(r'(.)\\1{4,}', ' ', x))\n",
    "    df_trans['description_combined_processed'] =  df_trans['description_combined_processed'].apply(lambda x: re.sub(' +',' ',x))\n",
    "    df_trans.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # examples where the description and the merchant name are the same are probably not too informative\n",
    "    df_trans = df_trans[df_trans['merchant_name_combined']!=df_trans['original_description_plaid']]\n",
    "\n",
    "\n",
    "    # create some sentences\n",
    "    df_trans['amount'] = df_trans['amount'].round(1)\n",
    "    df_trans['str_amount'] = df_trans['amount'].apply(lambda x: str(x))\n",
    "    df_trans['sentence'] = df_trans['description_combined_processed'] + '. Channel: ' + df_trans['payment_channel_processed'] + '. Amount: ' + df_trans['str_amount']\n",
    "    df_trans['sentence2'] = df_trans['description_combined_processed'] +'. Type: ' +df_trans['counterparty_type']+'. Channel: ' +\\\n",
    "        df_trans['payment_channel_processed'] + '. Amount: ' + df_trans['str_amount']\n",
    "    try:\n",
    "        df_trans.drop('str_amount', axis=1, inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "    df_trans['len_sentence'] = df_trans['sentence'].apply(lambda x: len(x))\n",
    "    df_trans['num_words'] = df_trans['sentence'].apply(lambda x: len(x.split(' ')))\n",
    "\n",
    "    # There are some cases where Chime is the merchant but it isn't mentioned in the description, so remove these. I am sure there are more like these, we would need to see\n",
    "    df_trans['Chime in descr'] = df_trans['original_description_plaid'].apply(lambda x: 'Chime' in x)\n",
    "\n",
    "    df_trans = df_trans[(df_trans['Chime in descr'] & (df_trans['merchant_name_combined']=='Chime')) | (df_trans['merchant_name_combined']!='Chime')]\n",
    "    df_trans.drop('Chime in descr', axis=1, inplace=True)\n",
    "    df_trans.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    df_trans_no_merchant = df_trans[(df_trans['merchant_name_combined'].isnull()) | df_trans['merchant_name_combined'].isin(['',' '])]\n",
    "\n",
    "    # examples where the description and the merchant name are the same are probably not too informative\n",
    "    df_trans = df_trans[df_trans['merchant_name_combined']!=df_trans['original_description_plaid']]\n",
    "    print(df_trans.shape)\n",
    "    #df_trans_cln3['merchant_name_combined'].value_counts()[:-40]\n",
    "    df_trans = df_trans.drop_duplicates(subset=['original_description_plaid','merchant_name_combined'])\n",
    "    df_trans.shape\n",
    "    #pre-processing about halves the volume of data\n",
    "\n",
    "\n",
    "    columns_to_keep = ['transaction_id','corrected_made_on','amount','description_combined','merchant_name_combined','description_combined_processed','sentence','sentence2','payment_channel','currency_code','original_description_plaid','description']\n",
    "    nunique_merchants = df_trans['merchant_name_combined'].nunique()\n",
    "    print(f\"There are {nunique_merchants} merchants in the dataset\")\n",
    "    df_trans['merchant_name_combined'].value_counts(dropna=False)\n",
    "    path_file_processed_p = path_file_processed+'.parquet'\n",
    "    print(path_file_processed_p)\n",
    "    wr.s3.to_parquet(\n",
    "        df=df_trans[columns_to_keep],\n",
    "        path=path_file_processed,\n",
    "        dataset=True #,partition_cols=[\"merchant_name_combined\"]\n",
    "    )\n",
    "\n",
    "    print(f\"Finsihed writing file {path_file_processed_p}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
